{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "4c7c59fe"
   },
   "source": [
    "`torch.cuda.get_device_capability()` 함수를 사용하여 현재 CUDA 장치의 major 버전과 minor 버전을 조회합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b47d8c1",
    "outputId": "524d33e8-7f4f-4cc0-da0d-85a294d893ec"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# CUDA 장치의 주요 버전과 부 버전을 가져옵니다.\n",
    "major_version, minor_version = torch.cuda.get_device_capability()\n",
    "major_version, minor_version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "b7897823"
   },
   "source": [
    "`unsloth` 라이브러리와 관련 디펜던시를 설치하는 과정을 설명합니다.\n",
    "\n",
    "- Colab 환경에서 `torch` 버전 2.2.1과 호환되지 않는 패키지를 회피하기 위해 `unsloth` 라이브러리를 별도로 설치합니다.\n",
    "- GPU의 종류(신형 또는 구형)에 따라 조건부로 필요한 패키지들을 설치합니다.\n",
    "  - 신형 GPU(Ampere, Hopper 등)의 경우, `packaging`, `ninja`, `einops`, `flash-attn`, `xformers`, `trl`, `peft`, `accelerate`, `bitsandbytes` 패키지를 의존성 없이 설치합니다.\n",
    "  - 구형 GPU(V100, Tesla T4, RTX 20xx 등)의 경우, `xformers`, `trl`, `peft`, `accelerate`, `bitsandbytes` 패키지를 의존성 없이 설치합니다.\n",
    "- 설치 과정에서 발생하는 출력을 숨기기 위해 `%%capture` 매직 커맨드를 사용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "id": "f2728224"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Colab에서 torch 2.2.1을 사용하고 있으므로, 패키지 충돌을 방지하기 위해 별도로 설치해야 합니다.\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "if major_version >= 8:\n",
    "    # 새로운 GPU(예: Ampere, Hopper GPUs - RTX 30xx, RTX 40xx, A100, H100, L40)에 사용하세요.\n",
    "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
    "else:\n",
    "    # 오래된 GPU(예: V100, Tesla T4, RTX 20xx)에 사용하세요.\n",
    "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "id": "0c110a57"
   },
   "source": [
    "## Unsloth\n",
    "\n",
    "- `Unsloth`는 Llama, Mistral, CodeLlama, TinyLlama, Vicuna, Open Hermes 등을 지원합니다. 그리고 Yi, Qwen([llamafied](https://huggingface.co/models?sort=trending&search=qwen+llama)), Deepseek, 모든 Llama, Mistral 파생 아키텍처도 지원합니다.\n",
    "\n",
    "- `Unsloth`는 16비트 LoRA 또는 4비트 QLoRA를 지원합니다. 둘 다 2배 빠릅니다.\n",
    "\n",
    "- `max_seq_length`는 [kaiokendev의](https://kaiokendev.github.io/til) 방법을 통해 자동으로 RoPE 스케일링을 하기 때문에 어떤 값으로도 설정할 수 있습니다.\n",
    "\n",
    "**새로운 소식**!\n",
    "\n",
    "- [PR 26037](https://github.com/huggingface/transformers/pull/26037)을 통해, 우리는 4비트 모델을 **4배 빠르게** 다운로드할 수 있는 기능을 지원합니다! [Unsloth Repository](https://huggingface.co/unsloth)에는 Llama, Mistral 4비트 모델이 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "id": "79e77752"
   },
   "source": [
    "`FastLanguageModel.from_pretrained` 함수를 사용하여 사전 훈련된 언어 모델을 로드하는 과정을 설명합니다.\n",
    "\n",
    "- 최대 시퀀스 길이(`max_seq_length`)를 설정하여 모델이 처리할 수 있는 입력 데이터의 길이를 지정합니다.\n",
    "- 데이터 타입(`dtype`)은 자동 감지되거나, 특정 하드웨어에 최적화된 형식(`Float16`, `Bfloat16`)으로 설정할 수 있습니다.\n",
    "- 4비트 양자화(`load_in_4bit`) 옵션을 사용하여 메모리 사용량을 줄일 수 있으며, 이는 선택적입니다.\n",
    "- 사전 정의된 4비트 양자화 모델 목록(`fourbit_models`)에서 선택하여 다운로드 시간을 단축하고 메모리 부족 문제를 방지할 수 있습니다.\n",
    "- `FastLanguageModel.from_pretrained` 함수를 통해 모델과 토크나이저를 로드하며, 이때 모델 이름(`model_name`), 최대 시퀀스 길이, 데이터 타입, 4비트 로딩 여부를 매개변수로 전달합니다.\n",
    "- 선택적으로, 특정 게이트 모델을 사용할 경우 토큰(`token`)을 제공할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178,
     "referenced_widgets": [
      "1d9eed40e7374b41952e7a70c6dd0d73",
      "ae97c864b6dd41319a9f3504c7ddb46a",
      "fc2d5d6dd75d47b096c8890d532eb5f7",
      "f59a94bc873c42ffb02fae8b0f09abff",
      "23f0e198678e4031bae61f6c7fb978f6",
      "bcc19d7e24784d0bbf421170ffbe17db",
      "0b46fdf024904e0bb71c91b1c6ad06e1",
      "b133d5f2f8564dce94ef3dffcb497dbd",
      "213d1429b122411b92402b1db64edf56",
      "032e9f44b8d94767aa705514d0025dad",
      "d5c586051fd9419cb3defa1e197fdcc0"
     ]
    },
    "id": "ebaf5ef0",
    "outputId": "7610ab5d-674e-46f4-a16a-1b6981bb2044"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 4096  # 최대 시퀀스 길이를 설정합니다. 내부적으로 RoPE 스케일링을 자동으로 지원합니다!\n",
    "# 자동 감지를 위해 None을 사용합니다. Tesla T4, V100은 Float16, Ampere+는 Bfloat16을 사용하세요.\n",
    "dtype = None\n",
    "# 메모리 사용량을 줄이기 위해 4bit 양자화를 사용합니다. False일 수도 있습니다.\n",
    "load_in_4bit = True\n",
    "\n",
    "# 4배 빠른 다운로드와 메모리 부족 문제를 방지하기 위해 지원하는 4bit 사전 양자화 모델입니다.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-it-bnb-4bit\",  # Gemma 7b의 Instruct 버전\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2b-it-bnb-4bit\",  # Gemma 2b의 Instruct 버전\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",  # Llama-3 8B\n",
    "]  # 더 많은 모델은 https://huggingface.co/unsloth 에서 확인할 수 있습니다.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    model_name=\"beomi/Llama-3-Open-Ko-8B-Instruct-preview\",  # 모델 이름을 설정합니다.\n",
    "    max_seq_length=max_seq_length,  # 최대 시퀀스 길이를 설정합니다.\n",
    "    dtype=dtype,  # 데이터 타입을 설정합니다.\n",
    "    load_in_4bit=load_in_4bit,  # 4bit 양자화 로드 여부를 설정합니다.\n",
    "    # token = \"hf_...\", # 게이트된 모델을 사용하는 경우 토큰을 사용하세요. 예: meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "id": "2c0fe045"
   },
   "source": [
    "이제 LoRA 어댑터를 추가하여 모든 파라미터 중 단 1% ~ 10%의 파라미터만 업데이트하면 됩니다!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "58c719fd"
   },
   "source": [
    "FastLanguageModel을 사용하여 특정 모듈에 대한 성능 향상 기법을 적용한 모델을 구성합니다.\n",
    "\n",
    "- `FastLanguageModel.get_peft_model` 함수를 호출하여 모델을 초기화하고, 성능 향상을 위한 여러 파라미터를 설정합니다.\n",
    "- `r` 파라미터를 통해 성능 향상 기법의 강도를 조절합니다. 권장 값으로는 8, 16, 32, 64, 128 등이 있습니다.\n",
    "- `target_modules` 리스트에는 성능 향상을 적용할 모델의 모듈 이름들이 포함됩니다.\n",
    "- `lora_alpha`와 `lora_dropout`을 설정하여 LoRA(Low-Rank Adaptation) 기법의 세부 파라미터를 조정합니다.\n",
    "- `bias` 옵션을 통해 모델의 바이어스 사용 여부를 설정할 수 있으며, 최적화를 위해 \"none\"으로 설정하는 것이 권장됩니다.\n",
    "- `use_gradient_checkpointing` 옵션을 \"unsloth\"로 설정하여 VRAM 사용량을 줄이고, 더 큰 배치 크기로 학습할 수 있도록 합니다.\n",
    "- `use_rslora` 옵션을 통해 Rank Stabilized LoRA를 사용할지 여부를 결정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "id": "f990387e"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # 0보다 큰 어떤 숫자도 선택 가능! 8, 16, 32, 64, 128이 권장됩니다.\n",
    "    lora_alpha=32,  # LoRA 알파 값을 설정합니다.\n",
    "    lora_dropout=0.05,  # 드롭아웃을 지원합니다.\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],  # 타겟 모듈을 지정합니다.\n",
    "    bias=\"none\",  # 바이어스를 지원합니다.\n",
    "    # True 또는 \"unsloth\"를 사용하여 매우 긴 컨텍스트에 대해 VRAM을 30% 덜 사용하고, 2배 더 큰 배치 크기를 지원합니다.\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=123,  # 난수 상태를 설정합니다.\n",
    "    use_rslora=False,  # 순위 안정화 LoRA를 지원합니다.\n",
    "    loftq_config=None,  # LoftQ를 지원합니다.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "id": "1cd96e8b"
   },
   "source": [
    "### 데이터 준비\n",
    "\n",
    "**[중요]**\n",
    "\n",
    "- 토큰화된 출력에 **EOS_TOKEN**을 추가하는 것을 잊지 마세요! 그렇지 않으면 무한 생성이 발생할 수 있습니다.\n",
    "\n",
    "**[참고]**\n",
    "\n",
    "- 오직 완성된 텍스트만을 학습하고자 한다면, TRL의 문서를 [여기](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only)에서 확인하세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "7dd387ec"
   },
   "source": [
    "`load_dataset` 함수를 사용하여 특정 데이터셋을 로드하고, 이를 특정 형식으로 포매팅하는 과정을 설명합니다.\n",
    "\n",
    "- `load_dataset` 함수로 \"teddylee777/QA-Dataset-mini\" 데이터셋을 \"train\" 분할로 로드합니다.\n",
    "- 데이터셋의 각 예제에 대해 `formatting_prompts_func` 함수를 적용하여 포매팅을 수행합니다.\n",
    "  - 이 함수는 \"instruction\"과 \"output\" 필드를 사용하여 주어진 포맷에 맞게 텍스트를 재구성합니다.\n",
    "  - 재구성된 텍스트는 `alpaca_prompt` 포맷을 따르며, 각 항목의 끝에는 `EOS_TOKEN`을 추가하여 생성이 종료되도록 합니다.\n",
    "- 최종적으로, 포매팅된 텍스트는 \"text\" 키를 가진 딕셔너리 형태로 반환됩니다.\n",
    "- 이 과정을 통해, AI 모델이 처리하기 적합한 형태로 데이터를 전처리하는 방법을 보여줍니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "c45a6628633c4596b1acb0a467b46346",
      "456395ce51864b8aa19575d7447f7dc3",
      "232306240fe948b388d0c994d1bd5a4c",
      "16156f030f9949c290548cc53fdff39e",
      "889691ef9a24475b8dc3d6e91521f708",
      "4cf2c580b7694a54afd26d30768b2335",
      "472f4ae605dd48339d0e70d8f421e1a8",
      "fb932c0e50e7444b89ad910738d9aac4",
      "ca994ec63f924e9aa368cdd85cc1f29c",
      "f8082474f8fb4f8e8ae20208b59a78b6",
      "a3706633507040f2ab6851752c02f8fa"
     ]
    },
    "id": "1eb5d1bf",
    "outputId": "688870a9-05ee-4779-f850-fc69a61dc939"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# EOS_TOKEN은 문장의 끝을 나타내는 토큰입니다. 이 토큰을 추가해야 합니다.\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "# AlpacaPrompt를 사용하여 지시사항을 포맷팅하는 함수입니다.\n",
    "alpaca_prompt = \"\"\"Below is QA datasets that describes contents of NAVER news. Write a response that appropriately completes the request.\n",
    "\n",
    "### QUESTION:\n",
    "{}\n",
    "\n",
    "### ANSWER:\n",
    "{}\"\"\"\n",
    "\n",
    "# 주어진 예시들을 포맷팅하는 함수입니다.\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"QUESTION\"]  # 지시사항 리스트\n",
    "    outputs = examples[\"ANSWER\"]  # 출력 리스트\n",
    "    texts = []\n",
    "    for q, a in zip(instructions, outputs):\n",
    "        # EOS_TOKEN을 추가해야 합니다. 그렇지 않으면 생성이 무한히 진행될 수 있습니다.\n",
    "        text = alpaca_prompt.format(q, a) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# 내 데이터셋 로드\n",
    "dataset = load_dataset(\"HaGPT/my-news-qa-dataset\", split=\"train\")\n",
    "\n",
    "# 데이터셋에 formatting_prompts_func 함수를 적용\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "id": "4c290b06"
   },
   "source": [
    "### 모델 훈련하기\n",
    "\n",
    "이제 Huggingface TRL의 `SFTTrainer`를 사용해 봅시다!\n",
    "\n",
    "- 참고 문서: [TRL SFT 문서](https://huggingface.co/docs/trl/sft_trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "id": "b41b8fd9"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "tokenizer.padding_side = \"right\"  # 토크나이저의 패딩을 오른쪽으로 설정합니다.\n",
    "\n",
    "# SFTTrainer를 사용하여 모델 학습 설정\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # 학습할 모델\n",
    "    tokenizer=tokenizer,  # 토크나이저\n",
    "    train_dataset=dataset,  # 학습 데이터셋\n",
    "    eval_dataset=dataset,\n",
    "    dataset_text_field=\"text\",  # 데이터셋에서 텍스트 필드의 이름\n",
    "    max_seq_length=max_seq_length,  # 최대 시퀀스 길이\n",
    "    dataset_num_proc=2,  # 데이터 처리에 사용할 프로세스 수\n",
    "    packing=False,  # 짧은 시퀀스에 대한 학습 속도를 5배 빠르게 할 수 있음\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,  # 각 디바이스당 훈련 배치 크기\n",
    "        gradient_accumulation_steps=4,  # 그래디언트 누적 단계\n",
    "        warmup_steps=5,  # 웜업 스텝 수\n",
    "        num_train_epochs=3,  # 훈련 에폭 수\n",
    "        max_steps=100,  # 최대 스텝 수\n",
    "        do_eval=True,\n",
    "        eval_strategy=\"steps\",\n",
    "        logging_steps=1,  # logging 스텝 수\n",
    "        learning_rate=2e-4,  # 학습률\n",
    "        fp16=not torch.cuda.is_bf16_supported(),  # fp16 사용 여부, bf16이 지원되지 않는 경우에만 사용\n",
    "        bf16=torch.cuda.is_bf16_supported(),  # bf16 사용 여부, bf16이 지원되는 경우에만 사용\n",
    "        optim=\"adamw_8bit\",  # 최적화 알고리즘\n",
    "        weight_decay=0.01,  # 가중치 감소\n",
    "        lr_scheduler_type=\"cosine\",  # 학습률 스케줄러 유형\n",
    "        seed=123,  # 랜덤 시드\n",
    "        output_dir=\"outputs\",  # 출력 디렉토리\n",
    "        report_to= \"none\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "id": "a7e10974"
   },
   "source": [
    "- GPU의 현재 메모리 상태를 확인합니다.\n",
    "- `torch.cuda.get_device_properties(0)`를 사용하여 첫 번째 GPU의 속성을 조회합니다.\n",
    "- `torch.cuda.max_memory_reserved()`를 통해 현재 예약된 최대 메모리를 GB 단위로 계산합니다.\n",
    "- GPU의 총 메모리 크기를 GB 단위로 계산합니다.\n",
    "- GPU 이름과 최대 메모리 크기, 현재 예약된 메모리 크기를 출력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b91c701f",
    "outputId": "0c083c56-19d4-4d8d-ac23-fe5c3385df8e"
   },
   "outputs": [],
   "source": [
    "# 현재 메모리 상태를 보여주는 코드\n",
    "gpu_stats = torch.cuda.get_device_properties(0)  # GPU 속성 가져오기\n",
    "start_gpu_memory = round(\n",
    "    torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3\n",
    ")  # 시작 시 예약된 GPU 메모리 계산\n",
    "max_memory = round(\n",
    "    gpu_stats.total_memory / 1024 / 1024 / 1024, 3\n",
    ")  # GPU의 최대 메모리 계산\n",
    "print(\n",
    "    f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\"\n",
    ")  # GPU 이름과 최대 메모리 출력\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")  # 예약된 메모리 양 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "91622afd",
    "outputId": "3238b1f1-a365-40e2-ec66-ef5ea383a535"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()  # 모델을 훈련시키고 통계를 반환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "ac245967"
   },
   "source": [
    "PyTorch를 사용하여 모델 훈련 시 메모리 사용량과 훈련 시간을 계산하고 출력하는 코드입니다.\n",
    "\n",
    "- `torch.cuda.max_memory_reserved()`를 사용하여 훈련 중에 예약된 최대 GPU 메모리를 계산합니다.\n",
    "- 훈련 시작 시점의 GPU 메모리 사용량과 비교하여 LoRA(Low-Rank Adaptation)를 위해 사용된 추가 메모리 양을 계산합니다.\n",
    "- 전체 GPU 메모리 대비 사용된 메모리의 비율을 계산합니다.\n",
    "- 훈련에 소요된 총 시간을 초와 분 단위로 출력합니다.\n",
    "- 예약된 최대 메모리, LoRA를 위해 사용된 메모리, 그리고 이들이 전체 GPU 메모리 대비 차지하는 비율을 출력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0f17487f",
    "outputId": "fa7ce933-68c6-4617-a14f-667cd9472f83"
   },
   "outputs": [],
   "source": [
    "# 최종 메모리 및 시간 통계를 보여줍니다.\n",
    "used_memory = round(\n",
    "    torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3\n",
    ")  # 사용된 최대 메모리를 GB 단위로 계산합니다.\n",
    "used_memory_for_lora = round(\n",
    "    used_memory - start_gpu_memory, 3\n",
    ")  # LoRA를 위해 사용된 메모리를 GB 단위로 계산합니다.\n",
    "used_percentage = round(\n",
    "    used_memory / max_memory * 100, 3\n",
    ")  # 최대 메모리 대비 사용된 메모리의 비율을 계산합니다.\n",
    "lora_percentage = round(\n",
    "    used_memory_for_lora / max_memory * 100, 3\n",
    ")  # 최대 메모리 대비 LoRA를 위해 사용된 메모리의 비율을 계산합니다.\n",
    "print(\n",
    "    f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\"\n",
    ")  # 훈련에 사용된 시간을 초 단위로 출력합니다.\n",
    "print(\n",
    "    # 훈련에 사용된 시간을 분 단위로 출력합니다.\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(\n",
    "    f\"Peak reserved memory = {used_memory} GB.\"\n",
    ")  # 예약된 최대 메모리를 GB 단위로 출력합니다.\n",
    "print(\n",
    "    f\"Peak reserved memory for training = {used_memory_for_lora} GB.\"\n",
    ")  # 훈련을 위해 예약된 최대 메모리를 GB 단위로 출력합니다.\n",
    "print(\n",
    "    f\"Peak reserved memory % of max memory = {used_percentage} %.\"\n",
    ")  # 최대 메모리 대비 예약된 메모리의 비율을 출력합니다.\n",
    "print(\n",
    "    f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\"\n",
    ")  # 최대 메모리 대비 훈련을 위해 예약된 메모리의 비율을 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "id": "bdf87c13"
   },
   "source": [
    "### 추론\n",
    "\n",
    "모델을 실행해 봅시다! 지시사항과 입력값을 변경할 수 있으며, 출력값은 비워두세요!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "id": "605d8d21"
   },
   "source": [
    "`TextStreamer`를 사용하여 연속적인 추론을 수행할 수도 있습니다. 이를 통해 전체를 기다리는 대신 토큰별로 생성 결과를 확인할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "71019b2a"
   },
   "source": [
    "- `FastLanguageModel.for_inference(model)`을 호출하여 모델의 추론 속도를 2배 향상시킵니다.\n",
    "- `tokenizer`를 사용하여 특정 포맷의 프롬프트를 토큰화하고, 이를 CUDA 기반의 텐서로 변환합니다. 이 과정에서 피보나치 수열을 계속하는 지시문, 입력값, 그리고 출력값을 위한 빈 공간을 포함합니다.\n",
    "- `TextStreamer` 객체를 `tokenizer`와 함께 초기화하여 텍스트 스트리밍 기능을 활성화합니다.\n",
    "- `model.generate` 함수를 호출하여 주어진 입력에 대한 텍스트 생성을 수행합니다. 이때, 최대 128개의 새로운 토큰을 생성할 수 있도록 설정하고, `TextStreamer`를 사용하여 결과를 스트리밍합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "id": "d58adce5"
   },
   "source": [
    "`StoppingCriteria`와 `StoppingCriteriaList`를 사용하여 특정 토큰에서 생성을 중단하는 방법을 구현합니다.\n",
    "\n",
    "- `StopOnToken` 클래스는 `StoppingCriteria`를 상속받아, 생성 중 특정 토큰(`stop_token_id`)이 나타나면 생성을 중단하도록 합니다.\n",
    "- `stop_token` 변수에 중단할 토큰을 문자열로 지정합니다.\n",
    "- `tokenizer.encode` 메소드를 사용하여 `stop_token`을 해당 언어 모델의 토큰 ID로 변환합니다.\n",
    "- `StoppingCriteriaList`에 `StopOnToken` 인스턴스를 포함시켜, 생성 과정에서 이를 중단 조건으로 사용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "id": "bbfd6d48"
   },
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "\n",
    "class StopOnToken(StoppingCriteria):\n",
    "    def __init__(self, stop_token_id):\n",
    "        self.stop_token_id = stop_token_id  # 정지 토큰 ID를 초기화합니다.\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        return (\n",
    "            self.stop_token_id in input_ids[0]\n",
    "        )  # 입력된 ID 중 정지 토큰 ID가 있으면 정지합니다.\n",
    "\n",
    "\n",
    "# end_token을 설정\n",
    "stop_token = \"<|end_of_text|>\"  # end_token으로 사용할 토큰을 설정합니다.\n",
    "stop_token_id = tokenizer.encode(stop_token, add_special_tokens=False)[\n",
    "    0\n",
    "]  # end_token의 ID를 인코딩합니다.\n",
    "\n",
    "# Stopping criteria 설정\n",
    "stopping_criteria = StoppingCriteriaList(\n",
    "    [StopOnToken(stop_token_id)]\n",
    ")  # 정지 조건을 설정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "id": "_ZGu2HntqpdN"
   },
   "source": [
    "(예시 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f8441b0",
    "outputId": "e16e8956-e4ef-48c2-cde9-e4668cd6e62f"
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "# FastLanguageModel을 이용하여 추론 속도를 2배 빠르게 설정합니다.\n",
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        alpaca_prompt.format(\n",
    "            \"TIGER 코리아원자력 ETF’는 한국전력을 편입했나요? 아니라면 그 이유는 무엇인가요?\",  # 질문\n",
    "            \"\",  # 답\n",
    "        )\n",
    "    ],\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=4096,  # 최대 생성 토큰 수를 설정합니다.\n",
    "    stopping_criteria=stopping_criteria  # 생성을 멈출 기준을 설정합니다.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "id": "emWg-dCBqjZD"
   },
   "source": [
    "(예시2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6A62BryCpcST",
    "outputId": "730980ad-4a98-47f7-d4e9-cea3deabba57"
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    [\n",
    "        alpaca_prompt.format(\n",
    "            \"상장 후 한 달도 안 돼 ‘TIGER 코리아원자력 ETF’가 돌파한 순자산 규모의 기준선은 무엇인가요?\",  # 질문\n",
    "            \"\",  # 답\n",
    "        )\n",
    "    ],\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=4096,  # 최대 생성 토큰 수를 설정합니다.\n",
    "    stopping_criteria=stopping_criteria  # 생성을 멈출 기준을 설정합니다.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "id": "mtBSJJ0FqtxH"
   },
   "source": [
    "(예시 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LnTb41D9p8AS",
    "outputId": "ff740882-bd85-44c7-d258-e010c8066286"
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    [\n",
    "        alpaca_prompt.format(\n",
    "            \"‘퀵커머스’는 무엇을 의미하며, 일반적으로 어느 정도 시간 내 배송되나요?\",  # 질문\n",
    "            \"\",  # 답\n",
    "        )\n",
    "    ],\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=4096,  # 최대 생성 토큰 수를 설정합니다.\n",
    "    stopping_criteria=stopping_criteria  # 생성을 멈출 기준을 설정합니다.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163,
     "referenced_widgets": [
      "f08ae27ef1de41e1953132383a2d26c3",
      "34a106da6be64d1ba95748871b5b55d7",
      "bb2327be6cb8479089ed75c3338ca6e3",
      "2a61bf17a9bf40efb33bdddfa1908a67",
      "0a20ff82205c43f3862563b43f8b6ea6",
      "3877ed9a0a0044ef868b4c4c1481cd10",
      "b4f0fe69fa394b13be3cbd715a524fe2",
      "f62c2e1a3e1d4bbb892e0339f25bbe5e",
      "25b6f8b752994e6d8f9942dc63dd53cd",
      "5d14e4c9c06345188e9adf056bad2409",
      "d8815281547b4ef48dbb5b3e743dc0d8",
      "6541792c13614d6d9dc1dee197987870",
      "b82d499b7430400dbda538528e37f806",
      "53137f86b5e24616b424f99add75646a",
      "7b0046b7462d4b57a52c2cfa40ab8728",
      "1069f4192e63449487dc6090249b4bfc",
      "57586329b0694fcd86cd6053af9b3c6f",
      "6e55b0938d674e74880124eaa3170640",
      "8c36596b76bf456bab25a34f6f90aaf6",
      "103972a2c5e04659a7e8c6d1119a313f",
      "ad0821f61de3470c8f04c0babc33af4c",
      "811f66c995e441cfbe90e11320860e1e",
      "62de6d9526954cbda65887053847d01b",
      "3a6ee560a2db4dbca8a8b25898668351",
      "2cb5909acf3943549559785758e2223b",
      "d5bf3ab075164ea486cd5ba6f365fb13",
      "8d185441372644c9a608bbfad4c2caef",
      "c7fcde57db4f45c6876cb16e529156be",
      "7398d0aa5fe34eb5832bdc6ff6b56d8c",
      "e7ca4a89cf1c431b8509387a271345e9",
      "6097cb2113244f81a04ec0b3b60fea32",
      "10cd49c9cfc5433e83957b3ea24bd8f4",
      "5ac3449fde814687aed3e4a4457a14e9",
      "0224a08c00814992859ebc18d7f9d179",
      "02ae996f099247adb7bd6014191d5056",
      "3bc57b6223734929b49ca9ae9f65f1b7",
      "09e4435069c144c58420a12bad6e7e2c",
      "2d5100739a614a76bfb79dbaab43a488",
      "19fb9d2d190a4b3785980842f150e638",
      "e0f1341b8bb54eb3bc935310d677fe42",
      "ab47fae30a0042618ac5849d7dd68820",
      "647daa64709445e6940b3e42fccd5fb1",
      "866762cca0bb42128f182ae4b8aab393",
      "36e99ec47897488ebd04d0c18aeab8a8"
     ]
    },
    "id": "7425142b",
    "outputId": "f9b8a0dc-3c22-40cc-f20d-ba4a4df77536"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"Llama-3-Open-Ko-8B-naver-news\")  # 모델을 로컬에 저장합니다.\n",
    "model.push_to_hub(\"HaGPT/my-news-qa-dataset\", token = \"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\") # 모델을 온라인 허브에 저장합니다."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
